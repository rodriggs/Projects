{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Market Demand\n",
    "My stakeholders want to use some market data to make a business decision. However, the key factor in the market data has a lot of missing values in it. \n",
    "\n",
    "In this exercise, I built a Linear Regression model with some data pre-processing such as: \n",
    "* Data cleaning and removing outliers\n",
    "* Scaling\n",
    "* SelectKBest and PCA for feature selection\n",
    "\n",
    "\n",
    "# Challenge\n",
    "There is a challenge in this exercise and that is a lack of data. As you will see, I only have around 70 data points to train my model with, and that goes down even further with train-test-split. With the model, I will need to \"fill in\" around 120 data gaps with my model's prediction. The goal is to try my best to reduce the features needed to train the model as much as possible while at the same time prevent overfitting by leaving some data for testing purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set_context('notebook')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn import linear_model\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import cross_val_predict\n",
    "from sklearn.cross_validation import ShuffleSplit\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn import feature_selection\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'Market Data.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-e875932eba6c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mraw_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Market Data.xlsx'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\leungr1\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\pandas\\io\\excel.pyc\u001b[0m in \u001b[0;36mread_excel\u001b[1;34m(io, sheetname, header, skiprows, skip_footer, index_col, names, parse_cols, parse_dates, date_parser, na_values, thousands, convert_float, has_index_names, converters, engine, squeeze, **kwds)\u001b[0m\n\u001b[0;32m    168\u001b[0m     \"\"\"\n\u001b[0;32m    169\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m         \u001b[0mio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m     return io._parse_excel(\n",
      "\u001b[1;32mC:\\Users\\leungr1\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\pandas\\io\\excel.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, io, **kwds)\u001b[0m\n\u001b[0;32m    225\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxlrd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen_workbook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_contents\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 227\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxlrd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen_workbook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    228\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m             raise ValueError('Must explicitly set engine if not passing in'\n",
      "\u001b[1;32mC:\\Users\\leungr1\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\xlrd\\__init__.pyc\u001b[0m in \u001b[0;36mopen_workbook\u001b[1;34m(filename, logfile, verbosity, use_mmap, file_contents, encoding_override, formatting_info, on_demand, ragged_rows)\u001b[0m\n\u001b[0;32m    392\u001b[0m         \u001b[0mpeek\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfile_contents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mpeeksz\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 394\u001b[1;33m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    395\u001b[0m         \u001b[0mpeek\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpeeksz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m         \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'Market Data.xlsx'"
     ]
    }
   ],
   "source": [
    "raw_df = pd.read_excel('Market Data.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# I am removing Qatar because it's an extreme outlier in the model fitting\n",
    "raw_df.drop(raw_df[raw_df['Country'] == 'Qatar'].index[0],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Number of observations:',len(raw_df)\n",
    "print 'Number of gaps I have to fill:', len(raw_df[np.isnan(raw_df['Factor 8 IU per capita'])])\n",
    "raw_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some columns that are just missing too many data points. I can't use them in the model for sure, so the feature set is primarily limited by missing data points. I can't do an across the board dropna() because I will lose a large portion of my data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I was doing exploratory data analysis, I was interested in using the infant mortality rate, but it had several error values that needs to be cleaned, so I'm excluding this from the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# China and India is throwing off my model with populataion 5x that of United States. \n",
    "raw_df.sort_values(by='Population mid-2015 (millions)', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The following returns an error, so that means there's some invalid data, which I will remove later on.\n",
    "# raw_df['Percent Urban'] = raw_df['Percent Urban'].apply(float)\n",
    "# for i in raw_df['Infant Mortality Ratea'].index:\n",
    "#     try:\n",
    "#         float(raw_df['Infant Mortality Ratea'][i])\n",
    "#     except:\n",
    "#         print raw_df.ix[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping the Identified Problematic Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The index to remove from the data set includes identified outliers and invalid data points\n",
    "processed_df = copy.deepcopy(raw_df[~raw_df.index.isin([47,125, 36, 13])])\n",
    "processed_df['Infant Mortality Ratea'] = processed_df['Infant Mortality Ratea'].apply(float)\n",
    "len(processed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "processed_df.ix[:,:'Factor 8 IU per capita'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Further removing features that I know I won't use in the feature selection step\n",
    "processed_df = processed_df.ix[:,:'Factor 8 IU per capita'].drop(['Calculated Hemophilia Population',\n",
    "                                                                  'Survey Hemophilia Population',\n",
    "                                                                   ],\n",
    "                                                                  axis=1)\n",
    "processed_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "processed_df = processed_df.dropna(subset=['GNI per Capita ($US) 2014c'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Observations left after cleaning:',len(processed_df)\n",
    "print 'Observations available for training:',len(processed_df.dropna())\n",
    "print processed_df.columns\n",
    "processed_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Taking out the highly correlated features to avoid multi-collinearity\n",
    "cols = [\n",
    "#         'Births per 1,000 Population',\n",
    "        'Deaths per 1,000 Population',\n",
    "        'Net Migration Rate per 1,000',\n",
    "        'Population mid-2015 (millions)', \n",
    "#         'Population mid-2030',\n",
    "#         'Population mid-2050', \n",
    "        'Infant Mortality Ratea',\n",
    "#         'Total Fertility Rateb', \n",
    "#         'Percent of population < 15 years',\n",
    "#         'Percent of population > 65 years',\n",
    "        'GNI per Capita ($US) 2014c',\n",
    "        'Percent Urban'\n",
    "        ]\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.heatmap(data=processed_df[cols].corr(), annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.pairplot(data=processed_df[cols], size = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = processed_df.dropna()['Factor 8 IU per capita']\n",
    "figure, axes = plt.subplots(nrows=2, ncols=3, figsize=(18,16))\n",
    "axes[0,0].scatter(processed_df.dropna()[cols[0]],y)\n",
    "axes[0,0].set_xlabel(cols[0])\n",
    "axes[0,0].set_ylabel('Factor 8 IU per capita')\n",
    "axes[0,1].scatter(processed_df.dropna()[cols[1]],y)\n",
    "axes[0,1].set_xlabel(cols[1])\n",
    "axes[0,2].scatter(processed_df.dropna()[cols[2]],y)\n",
    "axes[0,2].set_xlabel(cols[2])\n",
    "axes[1,0].scatter(processed_df.dropna()[cols[3]],y)\n",
    "axes[1,0].set_xlabel(cols[3])\n",
    "axes[1,0].set_ylabel('Factor 8 IU per capita')\n",
    "axes[1,1].scatter(processed_df.dropna()[cols[4]],y)\n",
    "axes[1,1].set_xlabel(cols[4])\n",
    "axes[1,2].scatter(processed_df.dropna()[cols[5]],y)\n",
    "axes[1,2].set_xlabel(cols[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like Infant Mortality Rate is inversely related to our target. Let's create a new feature to make Infant Mortality Rate have a more linear relationship with the target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "Modifying a feature that is more linearly aligned with the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "processed_df['Inverse Infant Mortality Rate'] = 1.0 / processed_df['Infant Mortality Ratea']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(processed_df.dropna()['Inverse Infant Mortality Rate'],\n",
    "            processed_df.dropna()['Factor 8 IU per capita'], \n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, our available data for training (before train-test-split) is very small. We need to further shorten the feature list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cols = [\n",
    "        'Deaths per 1,000 Population',\n",
    "        'Net Migration Rate per 1,000',\n",
    "        'Population mid-2015 (millions)', \n",
    "        'Inverse Infant Mortality Rate',\n",
    "        'GNI per Capita ($US) 2014c',\n",
    "        'Percent Urban'\n",
    "        ]\n",
    "\n",
    "features = processed_df.dropna()[cols]\n",
    "target = processed_df.dropna()['Factor 8 IU per capita']\n",
    "selector = SelectKBest(k=2)\n",
    "selector.fit(features,target)\n",
    "\n",
    "for i in range(len(features.columns)):\n",
    "    print '{}: {}'.format(features.columns[i],selector.scores_[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like the two features \"Inverse Infant Mortality Rate\" and \"GNI per Capita ($US) 2014c\" won."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,6))\n",
    "plt.subplot(121)\n",
    "plt.scatter(processed_df.dropna()['Inverse Infant Mortality Rate'],\n",
    "            processed_df.dropna()['Factor 8 IU per capita'])\n",
    "plt.xlabel('Inverse Infant Mortality Rate')\n",
    "plt.ylabel('Factor 8 IU per capita')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.scatter(processed_df.dropna()['GNI per Capita ($US) 2014c'],\n",
    "            processed_df.dropna()['Factor 8 IU per capita'])\n",
    "plt.xlabel('GNI per Capita ($US) 2014c')\n",
    "plt.ylabel('Factor 8 IU per capita')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Data available for train-test-split:',len(processed_df.dropna())\n",
    "print 'Gaps I need to fill:',len(processed_df) - len(processed_df.dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's not much, but that's all there is available to me. Just have to ***very carefully*** explain to my stakeholders regarding the accuracy of the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = processed_df.dropna()[['Inverse Infant Mortality Rate',\n",
    "                                  'GNI per Capita ($US) 2014c'\n",
    "                                 ]]\n",
    "target = processed_df.dropna()['Factor 8 IU per capita']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, \n",
    "                                                    target, \n",
    "                                                    test_size=0.5, \n",
    "                                                    random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = linear_model.LinearRegression(fit_intercept=False)\n",
    "model.fit(x_train,y_train)\n",
    "\n",
    "train_pred = model.predict(x_train)\n",
    "test_pred = model.predict(x_test)\n",
    "residuals = (y_test - test_pred)\n",
    "\n",
    "print 'Training results:'\n",
    "print 'Coefficients:',model.coef_\n",
    "print 'R-Squared:',model.score(x_train,y_train)\n",
    "print 'Mean Absolute Error:', metrics.mean_absolute_error(y_train,train_pred)\n",
    "print 'Mean Squared Error:', metrics.mean_squared_error(y_train,train_pred)\n",
    "\n",
    "print ''\n",
    "print 'Test results:'\n",
    "print 'R-Squared of test data:', model.score(x_test,y_test)\n",
    "print 'Mean Absolute Error:', metrics.mean_absolute_error(y_test,test_pred)\n",
    "print 'Mean Squared Error:', metrics.mean_squared_error(y_test,test_pred)\n",
    "\n",
    "\n",
    "figure, axes = plt.subplots(nrows=2,ncols=2, figsize=(18,10))\n",
    "\n",
    "axes[0,0].plot(train_pred,'b',label='Train Prediction')\n",
    "axes[0,0].plot(y_train.values,'r',label='y_train')\n",
    "axes[0,0].set_title('Predictions vs Train Data')\n",
    "axes[0,0].legend()\n",
    "\n",
    "\n",
    "axes[1,0].plot(test_pred,'b',label='Test Prediction')\n",
    "axes[1,0].plot(y_test.values,'r',label='y_test')\n",
    "axes[1,0].set_title('Predictions vs Test Data')\n",
    "axes[1,0].legend()\n",
    "\n",
    "axes[0,1].set_title('Predictions vs Test Data')\n",
    "axes[0,1].scatter(x=test_pred,y=y_test)\n",
    "axes[0,1].set_xlabel('Predictions')\n",
    "axes[0,1].set_ylabel('y_test')\n",
    "axes[0,1].plot(y_test,y_test)\n",
    "\n",
    "axes[1,1].hist(residuals)\n",
    "axes[1,1].set_title('Histogram of residuals between the Predictions and Test Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions are not ideal. But there are only around 70 data points to work with, so I can't use too many features to train the model or else I might risk overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv_generator = ShuffleSplit(len(features),n_iter=20,test_size=.5,random_state=1)\n",
    "cv_scores = cross_val_score(linear_model.LinearRegression(),X=features,y=target,cv=cv_generator)\n",
    "cv_scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CV score seems to be in line with our train-test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA Transform\n",
    "Let's get transformed data to see which data set performs better in Linear Regression. The idea is that PCA will include aspects from all 6 of our features, whereas SelectKBest simply drops 4 of our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loading our x_train - y_test data\n",
    "cols = [\n",
    "        'Deaths per 1,000 Population',\n",
    "        'Net Migration Rate per 1,000',\n",
    "        'Population mid-2015 (millions)', \n",
    "        'Inverse Infant Mortality Rate',\n",
    "        'GNI per Capita ($US) 2014c',\n",
    "        'Percent Urban'\n",
    "        ]\n",
    "\n",
    "features = processed_df.dropna()[cols]\n",
    "target = processed_df.dropna()['Factor 8 IU per capita']\n",
    "\n",
    "# PCA works best with scaled data\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(features)\n",
    "features = scaler.transform(features)\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, \n",
    "                                                    target, \n",
    "                                                    test_size=0.5, \n",
    "                                                    random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# PCA transforming our train/test features\n",
    "pca = PCA(n_components=3)\n",
    "pca.fit(x_train)\n",
    "\n",
    "print pca.explained_variance_ratio_\n",
    "\n",
    "x_train_pca = pca.transform(x_train)\n",
    "x_test_pca = pca.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 3 components, I'm covering about 85% of the variance. While it's not ideal, I can't include more components because I just don't have enough data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = linear_model.LinearRegression(fit_intercept=False)\n",
    "model.fit(x_train_pca,y_train)\n",
    "\n",
    "train_pred = model.predict(x_train_pca)\n",
    "test_pred = model.predict(x_test_pca)\n",
    "residuals = (y_test - test_pred)\n",
    "\n",
    "print 'Training results:'\n",
    "print 'Coefficients:',model.coef_\n",
    "print 'R-Squared:',model.score(x_train_pca,y_train)\n",
    "print 'Mean Absolute Error:', metrics.mean_absolute_error(y_train,train_pred)\n",
    "print 'Mean Squared Error:', metrics.mean_squared_error(y_train,train_pred)\n",
    "\n",
    "print ''\n",
    "print 'Test results:'\n",
    "print 'R-Squared of test data:', model.score(x_test_pca,y_test)\n",
    "print 'Mean Absolute Error:', metrics.mean_absolute_error(y_test,test_pred)\n",
    "print 'Mean Squared Error:', metrics.mean_squared_error(y_test,test_pred)\n",
    "\n",
    "\n",
    "figure, axes = plt.subplots(nrows=2,ncols=2, figsize=(18,10))\n",
    "\n",
    "axes[0,0].plot(train_pred,'b',label='Train Prediction')\n",
    "axes[0,0].plot(y_train.values,'r',label='y_train')\n",
    "axes[0,0].set_title('Predictions vs Train Data')\n",
    "axes[0,0].legend()\n",
    "\n",
    "\n",
    "axes[1,0].plot(test_pred,'b',label='Test Prediction')\n",
    "axes[1,0].plot(y_test.values,'r',label='y_test')\n",
    "axes[1,0].set_title('Predictions vs Test Data')\n",
    "axes[1,0].legend()\n",
    "\n",
    "axes[0,1].set_title('Predictions vs Test Data')\n",
    "axes[0,1].scatter(x=test_pred,y=y_test)\n",
    "axes[0,1].set_xlabel('Predictions')\n",
    "axes[0,1].set_ylabel('y_test')\n",
    "axes[0,1].plot(y_test,y_test)\n",
    "\n",
    "axes[1,1].hist(residuals)\n",
    "axes[1,1].set_title('Histogram of residuals between the Predictions and Test Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously SelectKBest worked better with this data set. Let's use the Linear Regression model to predict the values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using model to forecast\n",
    "The model is used to predict the missing values in our target column.\n",
    "\n",
    "**Be sure to load the correct model from above before running the following code.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Gaps in my data set I am trying to fill:',len(processed_df) - len(processed_df.dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = processed_df[['Inverse Infant Mortality Rate',\n",
    "                         'GNI per Capita ($US) 2014c'\n",
    "                        ]]\n",
    "\n",
    "predictions = model.predict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,6))\n",
    "plt.plot(processed_df['Factor 8 IU per capita'],'r',label='Data')\n",
    "plt.plot(predictions,'b',label='Predictions')\n",
    "plt.xlim(xmax=len(predictions))\n",
    "plt.ylim(ymin=-1)\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our data has a lot of \"gaps\" in between, and that's what the model is trying to create predictions for to fill in these \"gaps\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_df = processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_df['Predictions'] = predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how my final model's prediction compare with available y_true!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chart_predictions = final_df.dropna(subset=['Factor 8 IU per capita'])['Predictions']\n",
    "y_true = final_df.dropna(subset=['Factor 8 IU per capita'])['Factor 8 IU per capita']\n",
    "residuals = y_true - chart_predictions\n",
    "\n",
    "print 'R-Squared:', metrics.r2_score(y_true,chart_predictions)\n",
    "print 'Mean Absolute Error:', metrics.mean_absolute_error(y_true,chart_predictions)\n",
    "print 'Mean Squared Error:', metrics.mean_squared_error(y_true,chart_predictions)\n",
    "\n",
    "plt.figure(figsize=(18,3))\n",
    "plt.plot(chart_predictions,'r',label='Predictions')\n",
    "plt.plot(y_true,'b',label='y_true')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure(figsize=(18,3))\n",
    "plt.hist(residuals)\n",
    "\n",
    "plt.figure(figsize=(18,3))\n",
    "plt.scatter(x=chart_predictions,y=y_true)\n",
    "plt.plot(y_true,y_true)\n",
    "plt.xlabel('Predictions')\n",
    "plt.ylabel('y_true')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
