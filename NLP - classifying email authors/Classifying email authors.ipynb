{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Just for fun and knowledge sharing, I made a NLP classifier to differentiate email authors among two of my colleagues. The emails are chosen by random and obvious clues such as email signatures are removed from my dataset. Surprisingly, the classifier worked quite well with minimal hyperparameters tweaking. \n",
    "\n",
    "The approach is to vectorize the email into bag-of-words using different vectorizers:\n",
    "* CountVectorizer with individual counts for each word\n",
    "* CountVectorizer with binary counts\n",
    "* TF-IDF\n",
    "\n",
    "Different algorithms also work better with different vectorizers. Random Forest, Multinomial Naive Bayes, and Bernoulli Naive Bayes were used and compared. \n",
    "\n",
    "Model performance was assessed simply with separate training and testing set. Hyperparameters tweaking with Cross Validation is not performed in this notebook (they're not the focus of this exercise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import codecs\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB,BernoulliNB\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pd.options.display.max_columns = 500\n",
    "pd.options.display.max_colwidth = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "j_train_path = 'data/j/training'\n",
    "j_test_path = 'data/j/testing'\n",
    "r_train_path = 'data/r/training'\n",
    "r_test_path = 'data/r/testing'\n",
    "\n",
    "j_train_files = os.listdir(j_train_path)\n",
    "j_test_files = os.listdir(j_test_path)\n",
    "r_train_files = os.listdir(r_train_path)\n",
    "r_test_files = os.listdir(r_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/j/training\\\\j1.txt'"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(j_train_path,'j1.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_data(path,files):\n",
    "    data=[]\n",
    "    for i in files:\n",
    "        f = codecs.open(os.path.join(path,i),'r',encoding='utf-8')\n",
    "        data.append(f.read())\n",
    "        f.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "j_train_data = read_data(j_train_path,j_train_files)\n",
    "j_test_data = read_data(j_test_path,j_test_files)\n",
    "r_train_data = read_data(r_train_path,r_train_files)\n",
    "r_test_data = read_data(r_test_path,r_test_files)\n",
    "\n",
    "df_train = pd.DataFrame()\n",
    "df_train['text'] = j_train_data+r_train_data\n",
    "df_train['label'] = ['j'] * 10 + ['r'] * 10\n",
    "df_test = pd.DataFrame()\n",
    "df_test['text'] = j_test_data + r_test_data\n",
    "df_test['label'] = ['j'] * 4 + ['r'] * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train = df_train['text']\n",
    "x_test = df_test['text']\n",
    "y_train = df_train['label']\n",
    "y_test = df_test['label']\n",
    "\n",
    "# vec = CountVectorizer(ngram_range=(1,3))\n",
    "# x_train_dtm = vec.fit_transform(x_train)\n",
    "# x_test_dtm = vec.transform(x_test)\n",
    "\n",
    "vec = CountVectorizer(ngram_range=(1,3),binary=True)\n",
    "x_train_dtm = vec.fit_transform(x_train)\n",
    "x_test_dtm = vec.transform(x_test)\n",
    "\n",
    "# vec = TfidfVectorizer(ngram_range=(1,3))\n",
    "# x_train_dtm = vec.fit_transform(x_train)\n",
    "# x_test_dtm = vec.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          j       1.00      0.75      0.86         4\n",
      "          r       0.80      1.00      0.89         4\n",
      "\n",
      "avg / total       0.90      0.88      0.87         8\n",
      "\n",
      "accuracy: 0.875\n"
     ]
    }
   ],
   "source": [
    "# model = MultinomialNB()\n",
    "# model = RandomForestClassifier()\n",
    "model = BernoulliNB()\n",
    "model.fit(x_train_dtm,y_train)\n",
    "\n",
    "pred = model.predict(x_test_dtm)\n",
    "print classification_report(y_test,pred)\n",
    "print 'accuracy:',accuracy_score(y_test,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          j       0.80      1.00      0.89         4\n",
      "          r       1.00      0.75      0.86         4\n",
      "\n",
      "avg / total       0.90      0.88      0.87         8\n",
      "\n",
      "accuracy: 0.875\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([('vec',vec),('NB',model)])\n",
    "pipe.fit(x_train,y_train)\n",
    "pred = pipe.predict(x_test)\n",
    "\n",
    "print classification_report(y_test,pred)\n",
    "print 'accuracy:',accuracy_score(y_test,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['j']\n",
      "[[ 0.73178455  0.26821545]]\n"
     ]
    }
   ],
   "source": [
    "blah = ['''\n",
    "Hi all,\n",
    "\n",
    "Dave asked Steve, Robert and myself to give a brief overview of the pull system to the SLT this morning.\n",
    "\n",
    "Attached is the deck.\n",
    "\n",
    "It went down well with great support from Dave and Steve. Positive feedback from Scott Collins, Zhorzh, Parish, Rebecca, Doug Blackburn.\n",
    "\n",
    "Thanks to you all,\n",
    "''']\n",
    "\n",
    "print pipe.predict(blah)\n",
    "print pipe.predict_proba(blah)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
